# Algorithm Explanation: How Image Quality Assessment Works

## Table of Contents

1. [Overview](#overview)
2. [The NIMA Architecture](#the-nima-architecture)
3. [Metrics Measured](#metrics-measured)
4. [Image Preprocessing Pipeline](#image-preprocessing-pipeline)
5. [Model Inference Process](#model-inference-process)
6. [Score Calculation Algorithm](#score-calculation-algorithm)
7. [Ranking System](#ranking-system)
8. [Complete Algorithm Flow](#complete-algorithm-flow)
9. [Mathematical Details](#mathematical-details)
10. [Why This Approach Works](#why-this-approach-works)

---

## Overview

This app uses **NIMA (Neural Image Assessment)**, a deep learning approach developed by Google Research for predicting image quality. Unlike traditional methods that output a single quality score, NIMA predicts a **probability distribution** over quality ratings, making it more robust and aligned with human perception.

### Key Innovation

NIMA doesn't just say "this image has quality 7.5" â€” it says "there's a 25% chance humans would rate this 7, 20% would rate it 8, 15% would rate it 6..." and then calculates the expected value.

---

## The NIMA Architecture

### Foundation Models

Our app uses **two independent NIMA models**:

1. **Technical Quality Model**

   - Based on pre-trained convolutional neural network (CNN)
   - Fine-tuned on the TID2013 dataset (technical image distortions)
   - Evaluates objective image quality factors

2. **Aesthetic Quality Model**
   - Same CNN architecture
   - Fine-tuned on the AVA dataset (Aesthetic Visual Analysis)
   - Evaluates subjective aesthetic appeal

### Why Two Models?

Technical quality and aesthetic appeal are **orthogonal dimensions**:

- A technically perfect image can be aesthetically boring
- An aesthetically pleasing image might have technical flaws (intentional blur, grain, etc.)

**Example**:

```
Image A (Sharp landscape):
  Technical: 9.2 (very sharp, low noise)
  Aesthetic: 5.1 (boring composition)

Image B (Artistic portrait with soft focus):
  Technical: 6.8 (intentional softness)
  Aesthetic: 8.9 (beautiful composition, emotion)
```

---

## Metrics Measured

### Technical Quality Metrics

The **technical model** evaluates objective quality factors:

#### 1. **Sharpness**

- Edge definition
- Focus accuracy
- Blur detection
- Resolution effectiveness

#### 2. **Noise Level**

- ISO noise
- Compression artifacts
- Color banding
- Grain patterns

#### 3. **Exposure**

- Brightness distribution
- Clipping (overexposure/underexposure)
- Dynamic range utilization
- Histogram balance

#### 4. **Color Accuracy**

- White balance
- Color cast
- Saturation levels
- Color space consistency

#### 5. **Compression Quality**

- JPEG artifacts
- Blocking effects
- Ringing artifacts
- Detail preservation

#### 6. **Distortions**

- Lens aberrations
- Perspective distortion
- Chromatic aberration
- Vignetting

### Aesthetic Quality Metrics

The **aesthetic model** evaluates subjective appeal factors:

#### 1. **Composition**

- Rule of thirds
- Leading lines
- Balance and symmetry
- Visual weight distribution
- Negative space usage

#### 2. **Subject Matter**

- Interest level
- Emotional impact
- Storytelling
- Subject clarity

#### 3. **Color Harmony**

- Color palette coherence
- Complementary colors
- Color psychology
- Mood and atmosphere

#### 4. **Lighting Quality**

- Direction and quality of light
- Shadows and highlights
- Mood creation
- Three-dimensionality

#### 5. **Creativity**

- Uniqueness
- Artistic expression
- Visual interest
- Innovation

#### 6. **Emotional Response**

- Viewer engagement
- Memorability
- Aesthetic pleasure
- Emotional resonance

---

## Image Preprocessing Pipeline

Before inference, each image undergoes standardized preprocessing:

### Step 1: Image Loading

```
Input: Image file (any format: JPEG, PNG, HEIC, etc.)
Process: Decode to raw pixel data
Output: Raw image buffer
```

### Step 2: Resizing

```
Input: Image of any size (e.g., 4000Ã—3000)
Process: Bilinear interpolation resize
Output: 224Ã—224 pixel image

Why 224Ã—224?
- Standard CNN input size
- Balance between detail and computation
- Matches pre-training dimensions
```

**Algorithm**:

```dart
// Bilinear interpolation preserves image quality
resized_image = resize(original_image,
  width: 224,
  height: 224,
  interpolation: BILINEAR
)
```

### Step 3: RGB Extraction

```
Input: Image with possible alpha channel (RGBA)
Process: Extract only Red, Green, Blue channels
Output: RGB image (3 channels)

Why remove alpha?
- Models trained on RGB data
- Alpha channel not relevant for quality
- Reduces input dimensions
```

### Step 4: Normalization

```
Input: Pixel values in range [0, 255]
Process: Divide each value by 255.0
Output: Float32 values in range [0.0, 1.0]

Why normalize?
- Neural networks work best with small values
- Prevents gradient explosion during training
- Matches model training preprocessing
```

**Mathematical Formula**:

```
normalized_pixel = original_pixel / 255.0

Example:
  R = 180 â†’ 0.7059
  G = 200 â†’ 0.7843
  B = 150 â†’ 0.5882
```

### Step 5: Tensor Reshaping

```
Input: Flat array of 150,528 values (224Ã—224Ã—3)
Process: Reshape to 4D tensor
Output: [1, 224, 224, 3]
  - 1: Batch size (single image)
  - 224: Height in pixels
  - 224: Width in pixels
  - 3: RGB channels
```

**Data Layout**:

```
Flat: [Râ‚€,Gâ‚€,Bâ‚€, Râ‚,Gâ‚,Bâ‚, ..., Râ‚…â‚€â‚â‚‡â‚…,Gâ‚…â‚€â‚â‚‡â‚…,Bâ‚…â‚€â‚â‚‡â‚…]
                    â†“
4D: [[[  [Râ‚€,Gâ‚€,Bâ‚€],   [Râ‚,Gâ‚,Bâ‚], ..., [Râ‚‚â‚‚â‚ƒ,Gâ‚‚â‚‚â‚ƒ,Bâ‚‚â‚‚â‚ƒ]  ],  â† Row 0
      [  [Râ‚‚â‚‚â‚„,Gâ‚‚â‚‚â‚„,Bâ‚‚â‚‚â‚„], ...                             ],  â† Row 1
      ...
      [  [...                         [Râ‚…â‚€â‚â‚‡â‚…,Gâ‚…â‚€â‚â‚‡â‚…,Bâ‚…â‚€â‚â‚‡â‚…]]  ]]] â† Row 223
```

---

## Model Inference Process

### Neural Network Architecture

Both models use a similar CNN architecture:

```
Input Image [224Ã—224Ã—3]
     â†“
[Convolutional Layers]
  â€¢ Feature extraction
  â€¢ Pattern recognition
  â€¢ Hierarchical learning
     â†“
[Pooling Layers]
  â€¢ Spatial dimension reduction
  â€¢ Translation invariance
     â†“
[Fully Connected Layers]
  â€¢ High-level reasoning
  â€¢ Score prediction
     â†“
[Softmax Layer]
  â€¢ Convert to probabilities
  â€¢ Sum to 1.0
     â†“
Output: [Pâ‚, Pâ‚‚, Pâ‚ƒ, Pâ‚„, Pâ‚…, Pâ‚†, Pâ‚‡, Pâ‚ˆ, Pâ‚‰, Pâ‚â‚€]
```

### What the Network Learns

The CNN learns to detect **patterns** at multiple scales:

**Low-level features** (early layers):

- Edges
- Corners
- Textures
- Colors

**Mid-level features** (middle layers):

- Shapes
- Objects
- Patterns
- Structures

**High-level features** (deep layers):

- Compositions
- Scenes
- Concepts
- Quality indicators

### Inference Execution

```python
# Pseudocode
def run_inference(preprocessed_image):
    # Input: [1, 224, 224, 3] tensor

    # Forward pass through neural network
    feature_maps = extract_features(preprocessed_image)

    # Global average pooling
    global_features = average_pool(feature_maps)

    # Fully connected layers
    quality_logits = fully_connected(global_features)

    # Softmax to get probabilities
    probabilities = softmax(quality_logits)

    # Output: [Pâ‚, Pâ‚‚, ..., Pâ‚â‚€]
    return probabilities
```

### Output Interpretation

The model outputs a **10-element probability distribution**:

```
Output tensor shape: [1, 10]

Example output:
[0.01, 0.02, 0.05, 0.10, 0.15, 0.25, 0.20, 0.12, 0.07, 0.03]
 â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘
 Pâ‚    Pâ‚‚    Pâ‚ƒ    Pâ‚„    Pâ‚…    Pâ‚†    Pâ‚‡    Pâ‚ˆ    Pâ‚‰    Pâ‚â‚€

Interpretation:
- 1% probability of rating 1 (terrible)
- 2% probability of rating 2 (very poor)
- 5% probability of rating 3 (poor)
- 10% probability of rating 4 (below average)
- 15% probability of rating 5 (average)
- 25% probability of rating 6 (above average) â† Peak
- 20% probability of rating 7 (good)
- 12% probability of rating 8 (very good)
- 7% probability of rating 9 (excellent)
- 3% probability of rating 10 (perfect)

Note: Probabilities sum to 1.0 (100%)
```

---

## Score Calculation Algorithm

### Mean Opinion Score (MOS)

The final quality score is the **expected value** of the probability distribution:

### Mathematical Formula

```
MOS = Î£ (Páµ¢ Ã— Ráµ¢)  for i = 1 to 10

Where:
  Páµ¢ = Probability of rating i
  Ráµ¢ = Rating value i
  MOS = Mean Opinion Score
```

### Step-by-Step Calculation

```
Given probabilities: [0.01, 0.02, 0.05, 0.10, 0.15, 0.25, 0.20, 0.12, 0.07, 0.03]

MOS = (0.01 Ã— 1) + (0.02 Ã— 2) + (0.05 Ã— 3) + (0.10 Ã— 4) + (0.15 Ã— 5) +
      (0.25 Ã— 6) + (0.20 Ã— 7) + (0.12 Ã— 8) + (0.07 Ã— 9) + (0.03 Ã— 10)

    = 0.01 + 0.04 + 0.15 + 0.40 + 0.75 + 1.50 + 1.40 + 0.96 + 0.63 + 0.30

    = 6.14

Final Score: 6.14 / 10
```

### Implementation

```dart
double calculateMOS(List<double> probabilities) {
  double score = 0.0;

  for (int i = 0; i < 10; i++) {
    // Rating values are 1-10 (i+1)
    // probabilities[i] corresponds to rating (i+1)
    score += probabilities[i] * (i + 1);
  }

  return score; // Returns value between 1.0 and 10.0
}
```

### Why Use Expected Value?

**Traditional Approach** (single prediction):

```
Model says: "Quality = 6"
Problem: What if humans disagree?
```

**NIMA Approach** (distribution):

```
Model says: "25% say 6, 20% say 7, 15% say 5..."
Expected value: 6.14
Advantage: Captures uncertainty and human variance
```

### Score Precision

Scores are displayed with **2 decimal places**:

```
Raw: 6.142857...
Displayed: 6.14

Why 2 decimals?
- Meaningful precision
- Easy to compare
- Professional appearance
```

---

## Ranking System

### Average Score Calculation

Each image receives **three scores**:

1. **Technical Score** (T): From technical model
2. **Aesthetic Score** (A): From aesthetic model
3. **Average Score** (S): Combined metric

```
S = (T + A) / 2

Example:
  Technical: 7.23
  Aesthetic: 6.45
  Average: (7.23 + 6.45) / 2 = 6.84
```

### Why Use Average?

The average gives **equal weight** to both dimensions:

```
Scenario 1: Technically perfect, aesthetically boring
  T = 9.5, A = 4.5 â†’ S = 7.0

Scenario 2: Balanced quality
  T = 7.0, A = 7.0 â†’ S = 7.0

Scenario 3: Artistic but technically flawed
  T = 5.5, A = 8.5 â†’ S = 7.0
```

All three have the **same average**, but different characteristics. Users can see both individual scores to understand why.

### Ranking Algorithm

**Step 1: Filter**

```
eligible_images = images.where(
  image => image.has_technical_score AND image.has_aesthetic_score
)
```

**Step 2: Calculate Averages**

```
for each image in eligible_images:
  image.average_score = (image.technical + image.aesthetic) / 2
```

**Step 3: Sort**

```
sorted_images = eligible_images.sort_by(
  score => score.average_score,
  order: DESCENDING
)
```

**Step 4: Select Top 3**

```
best_3 = sorted_images.take(3)

Result:
  Rank 1: Highest average score  ğŸ¥‡
  Rank 2: Second highest         ğŸ¥ˆ
  Rank 3: Third highest          ğŸ¥‰
```

### Sorting Algorithm Details

**Algorithm**: TimSort (Dart's default)

- **Time Complexity**: O(n log n)
- **Space Complexity**: O(n)
- **Stability**: Yes (maintains order for equal values)

**Example**:

```
Unsorted:
  Image A: 6.84
  Image B: 8.12
  Image C: 5.67
  Image D: 7.45
  Image E: 8.12 (same as B)

After sorting (descending):
  1. Image B: 8.12  ğŸ¥‡
  2. Image E: 8.12  ğŸ¥ˆ (stable: maintains original order)
  3. Image D: 7.45  ğŸ¥‰
  (Image A: 6.84)
  (Image C: 5.67)
```

### Tie-Breaking

When images have **equal average scores**:

- Maintains insertion order (stable sort)
- Both displayed with equal ranking visually
- First in list appears first in dialog

---

## Complete Algorithm Flow

### End-to-End Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: IMAGE SELECTION                                     â”‚
â”‚ User selects/captures image â†’ File object                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: PREPROCESSING (Background Isolate)                  â”‚
â”‚                                                              â”‚
â”‚ 2a. Load image bytes                                        â”‚
â”‚     File â†’ Uint8List                                        â”‚
â”‚                                                              â”‚
â”‚ 2b. Decode image                                            â”‚
â”‚     Uint8List â†’ Image object                                â”‚
â”‚                                                              â”‚
â”‚ 2c. Resize to 224Ã—224                                       â”‚
â”‚     Image(WÃ—H) â†’ Image(224Ã—224)                             â”‚
â”‚     Method: Bilinear interpolation                          â”‚
â”‚                                                              â”‚
â”‚ 2d. Extract RGB and normalize                               â”‚
â”‚     for each pixel(x,y):                                    â”‚
â”‚       R = pixel.red / 255.0                                 â”‚
â”‚       G = pixel.green / 255.0                               â”‚
â”‚       B = pixel.blue / 255.0                                â”‚
â”‚     Result: Float32List[150,528]                            â”‚
â”‚                                                              â”‚
â”‚ 2e. Reshape to 4D tensor                                    â”‚
â”‚     [150528] â†’ [1][224][224][3]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: TECHNICAL MODEL INFERENCE (Main Thread)             â”‚
â”‚                                                              â”‚
â”‚ 3a. Load model interpreter                                  â”‚
â”‚     technical_model.tflite â†’ Interpreter                    â”‚
â”‚                                                              â”‚
â”‚ 3b. Forward pass through CNN                                â”‚
â”‚     Input: [1, 224, 224, 3]                                 â”‚
â”‚     Process: Convolution, pooling, activation layers        â”‚
â”‚     Output: [1, 10] probability distribution                â”‚
â”‚                                                              â”‚
â”‚ 3c. Calculate technical MOS                                 â”‚
â”‚     T = Î£(Páµ¢ Ã— i) for i=1 to 10                            â”‚
â”‚     Example: T = 7.23                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: AESTHETIC MODEL INFERENCE (Main Thread)             â”‚
â”‚                                                              â”‚
â”‚ 4a. Use same preprocessed data                              â”‚
â”‚     Input: [1, 224, 224, 3]                                 â”‚
â”‚                                                              â”‚
â”‚ 4b. Forward pass through aesthetic CNN                      â”‚
â”‚     Different weights than technical model                  â”‚
â”‚     Output: [1, 10] probability distribution                â”‚
â”‚                                                              â”‚
â”‚ 4c. Calculate aesthetic MOS                                 â”‚
â”‚     A = Î£(Páµ¢ Ã— i) for i=1 to 10                            â”‚
â”‚     Example: A = 6.45                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: SCORE AGGREGATION                                   â”‚
â”‚                                                              â”‚
â”‚ Calculate average score:                                    â”‚
â”‚   S = (T + A) / 2                                           â”‚
â”‚   Example: S = (7.23 + 6.45) / 2 = 6.84                    â”‚
â”‚                                                              â”‚
â”‚ Store all three scores:                                     â”‚
â”‚   - Technical: 7.23                                         â”‚
â”‚   - Aesthetic: 6.45                                         â”‚
â”‚   - Average: 6.84                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: DISPLAY RESULTS                                     â”‚
â”‚                                                              â”‚
â”‚ Update UI with formatted scores:                            â”‚
â”‚   ğŸ”§ Technical: 7.23                                        â”‚
â”‚   ğŸ¨ Aesthetic: 6.45                                        â”‚
â”‚   â­ Average: 6.84                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: RANKING (When requested)                            â”‚
â”‚                                                              â”‚
â”‚ 7a. Filter analyzed images                                  â”‚
â”‚     Keep only images with both scores                       â”‚
â”‚                                                              â”‚
â”‚ 7b. Sort by average score                                   â”‚
â”‚     Order: Descending (highest first)                       â”‚
â”‚     Algorithm: TimSort O(n log n)                           â”‚
â”‚                                                              â”‚
â”‚ 7c. Select top 3                                            â”‚
â”‚     Rank 1: Best[0]   ğŸ¥‡                                    â”‚
â”‚     Rank 2: Best[1]   ğŸ¥ˆ                                    â”‚
â”‚     Rank 3: Best[2]   ğŸ¥‰                                    â”‚
â”‚                                                              â”‚
â”‚ 7d. Display in dialog                                       â”‚
â”‚     Show large previews with all scores                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Mathematical Details

### Probability Distribution Properties

The output probabilities form a valid **discrete probability distribution**:

```
Properties:
1. Non-negative: Páµ¢ â‰¥ 0 for all i
2. Normalized: Î£ Páµ¢ = 1.0
3. Discrete: 10 discrete ratings
4. Unimodal: Usually peaks at one rating
```

### Expected Value Calculation

The MOS is the **expected value** of a discrete random variable:

```
E[X] = Î£ xáµ¢ Â· P(X = xáµ¢)

Where:
  X = Random variable (quality rating)
  xáµ¢ = Possible values (1, 2, 3, ..., 10)
  P(X = xáµ¢) = Probability of rating xáµ¢
```

### Score Range

```
Minimum possible score: 1.0
  When Pâ‚ = 1.0, all others = 0
  MOS = 1.0 Ã— 1 = 1.0

Maximum possible score: 10.0
  When Pâ‚â‚€ = 1.0, all others = 0
  MOS = 1.0 Ã— 10 = 10.0

Practical range: 3.0 - 8.5
  Real images rarely reach extremes
  Most fall in the middle range
```

### Distribution Shape Analysis

```
Sharp peak (high confidence):
  [0, 0, 0.05, 0.10, 0.15, 0.55, 0.10, 0.05, 0, 0]
  Model is confident: likely rating 6
  MOS â‰ˆ 5.85

Broad distribution (uncertain):
  [0.05, 0.08, 0.12, 0.15, 0.20, 0.15, 0.12, 0.08, 0.03, 0.02]
  Model uncertain: could be 4, 5, or 6
  MOS â‰ˆ 4.67

Bimodal (conflicting signals):
  [0, 0, 0.05, 0.30, 0.10, 0.05, 0.10, 0.30, 0.05, 0.05]
  Some say 4, others say 8
  MOS â‰ˆ 6.00
```

---

## Why This Approach Works

### Advantages of NIMA

#### 1. **Aligned with Human Perception**

- Humans disagree on quality ratings
- Distribution captures this variance
- Expected value represents consensus

#### 2. **Robust to Edge Cases**

```
Ambiguous image:
  Traditional: Forced to pick single score
  NIMA: Shows distribution of opinions
```

#### 3. **Separation of Concerns**

```
Technical + Aesthetic independence:
  - Technical excellence â‰  aesthetic appeal
  - Both dimensions captured separately
  - Users see complete picture
```

#### 4. **Confidence Indication**

```
Sharp peak = High confidence
Broad distribution = Uncertainty
User can interpret accordingly
```

### Scientific Validation

The NIMA approach is based on:

- Published research: "NIMA: Neural Image Assessment" (Google, 2018)
- Trained on large datasets (AVA: 250,000+ images)
- Validated against human ratings
- Outperforms traditional metrics (PSNR, SSIM)

### Comparison with Traditional Metrics

```
PSNR (Peak Signal-to-Noise Ratio):
  âœ— Only measures pixel differences
  âœ— Doesn't match human perception
  âœ— No aesthetic consideration

SSIM (Structural Similarity Index):
  âœ— Better than PSNR but still limited
  âœ— No learning from human ratings
  âœ— No aesthetic assessment

NIMA (Neural Image Assessment):
  âœ“ Learned from human ratings
  âœ“ Captures perceptual quality
  âœ“ Separate technical/aesthetic models
  âœ“ Probability distribution output
  âœ“ State-of-the-art accuracy
```

---

## Summary

### The Complete Algorithm in One Sentence

**"Preprocess the image to 224Ã—224 RGB normalized tensor, pass it through two trained neural networks that output probability distributions over 1-10 ratings, calculate expected values (MOS) for technical and aesthetic scores, average them, and rank images by this combined metric."**

### Key Takeaways

1. **Two Models**: Technical quality (objective) + Aesthetic appeal (subjective)

2. **Preprocessing**: Resize â†’ RGB â†’ Normalize â†’ Reshape

3. **Inference**: CNN forward pass â†’ Probability distribution [Pâ‚...Pâ‚â‚€]

4. **Scoring**: MOS = Î£(Páµ¢ Ã— i) â†’ Value between 1-10

5. **Ranking**: Average = (Technical + Aesthetic) / 2 â†’ Sort descending

6. **Display**: Show top 3 with medals ğŸ¥‡ğŸ¥ˆğŸ¥‰

### Performance Characteristics

```
Time Complexity:
  - Preprocessing: O(W Ã— H) â‰ˆ 1 second
  - Inference: O(1) â‰ˆ 1 second per model
  - Scoring: O(10) = O(1) â‰ˆ instant
  - Ranking: O(n log n) â‰ˆ instant for n=10
  - Total per image: ~2-3 seconds

Space Complexity:
  - Model size: 5 MB each (10 MB total)
  - Image tensor: 224Ã—224Ã—3Ã—4 bytes = 602 KB
  - Total: ~11 MB
```

---

## References

1. **NIMA Paper**: "NIMA: Neural Image Assessment" by Talebi & Milanfar (Google Research, 2018)

   - [arXiv:1709.05424](https://arxiv.org/abs/1709.05424)

2. **AVA Dataset**: "AVA: A Large-Scale Database for Aesthetic Visual Analysis"

   - 250,000+ images with aesthetic ratings

3. **TID2013 Dataset**: Tampere Image Database 2013

   - Technical quality assessment benchmark

4. **Implementation**: TensorFlow Lite models by Sophie Berger
   - [GitHub Repository](https://github.com/SophieMBerger/TensorFlow-Lite-implementation-of-Google-NIMA)

---

**This algorithm represents the state-of-the-art in automated image quality assessment, combining deep learning, human perception modeling, and practical engineering to deliver accurate, interpretable quality scores.**
